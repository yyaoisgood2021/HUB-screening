{"cells":[{"cell_type":"markdown","metadata":{"id":"O94AlFDJugsF"},"source":["This part was not included in the manuscript.\n","use this notebook to build LR models for hub essentiality classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1PwSxMrOVag"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import os, scipy\n","import networkx as nx\n","import seaborn as sns\n","\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn.cluster import KMeans\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","\n","from sklearn.metrics import (\n","    balanced_accuracy_score,\n","    average_precision_score,\n","    confusion_matrix,\n","    f1_score,\n","    classification_report,\n","    recall_score,\n","    roc_auc_score,\n","    auc,\n","    precision_score,\n","    precision_recall_curve,\n","    )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dt = pd.read_csv('resources/all_hubs.for_LR.txt', sep='\\t', )\n","dt = dt[dt['GC']>=0.4] # only study data with GC>0.4\n","# dt \n","\n","print((dt['Ess']=='Ess').sum()) # 91\n","print((dt['Ess']=='Noness').sum()) # 548\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# add seq PCs\n","PC_list = ['PC{}'.format(i) for i in range(1,31)] # top 30 PCs\n","dt_pc_1 = pd.read_csv('resources/ext_0.rep_2/hub_PC30.1st.withGC.1.txt', sep='\\t')\n","dt_pc_1 = dt_pc_1[['str']+PC_list].copy()\n","# dt_pc_1\n","\n","dt_pc_2 = pd.read_csv('resources/ext_0.rep_2/hub_PC30.2nd.0.txt', sep='\\t')\n","dt_pc_2 = dt_pc_2[['str']+PC_list].copy()\n","# dt_pc_2\n","\n","dt_1 = dt[dt['round']=='1st'].copy().reset_index(drop=True)\n","dt_2 = dt[dt['round']=='2nd'].copy().reset_index(drop=True)\n","\n","# print(dt_1.merge(dt_pc_1, on='str').iloc[:5,])\n","# print(dt_2.merge(dt_pc_2, on='str').iloc[:5,])\n","\n","dt = pd.concat((dt_1.merge(dt_pc_1, on='str'), dt_2.merge(dt_pc_2, on='str')), ignore_index=True)\n","N_original = len(dt.columns)\n","\n","\n","str_of_tested_hubs = list(dt['str'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSKj8FUXjtTi"},"outputs":[],"source":["# all features (peak + PR)\n","feature_cols = ['CTCF.narrow.rep-1', 'CTCF.narrow.rep-2',\n","       'RAD21.narrow.rep-1', 'RAD21.narrow.rep-2', 'SMC3.narrow.rep-1',\n","       'H3K27ac.narrow.rep-1', 'H3K27me3.narrow.rep-1', 'H3K27me3.broad.rep-2',\n","       'H3K36me3.narrow.rep-1', 'H3K36me3.narrow.rep-2',\n","       'H3K36me3.broad.rep-3', 'H3K4me1.narrow.rep-1', 'H3K4me1.narrow.rep-2',\n","       'H3K4me2.narrow.rep-1', 'H3K4me3.narrow.rep-1', 'H3K4me3.narrow.rep-2',\n","       'H3K9ac.narrow.rep-1', 'H3K9ac.narrow.rep-2', 'H3K9me3.narrow.rep-1',\n","       'H3K9me3.broad.rep-2', 'H4K20me1.narrow.rep-1', 'ATAC',\n","       'ess_gene.Morgens', 'ess_gene.Wang', 'lncRNA.Liu', 'all_gene'\n","       ] \n","\n","feature_cols_for_PR = ['pr_mean_{}.scaled'.format(i) for i in feature_cols] + ['pr_mean_default.scaled']\n","\n","chrid_list = ['chr{}'.format(i) for i in range(1,9)] + ['chr{}'.format(i) for i in range(10,22)] +['chrX'] + ['der9' + 'phil22']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# add \"scaled PR info\" and \"peak info\" to dt\n","\n","for i, chrid in enumerate(chrid_list):\n","    pr_result_new = pd.read_csv('PR-LR/PR_scores/proc/result.{}.1.txt'.format(chrid), sep='\\t', )\n","    pr_result_new = pr_result_new[['str']+feature_cols_for_PR]\n","    \n","    pk_result = pd.read_csv('PR-LR/node_meta.1/{}.txt'.format(chrid), sep='\\t',)\n","    pk_result = pk_result[['str']+feature_cols]\n","    if i==0:\n","        pr_all = pr_result_new.copy()\n","        pk_all = pk_result.copy()\n","    else:\n","        pr_all = pd.concat((pr_all, pr_result_new), ignore_index=True, )\n","        pk_all = pd.concat((pk_all, pk_result), ignore_index=True, )\n","\n","\n","dt = dt.merge(pr_all, how='left', on='str')\n","dt = dt.merge(pk_all, how='left', on='str')\n","\n","# fill na using mean\n","dt.iloc[:,N_original:] = dt.iloc[:,N_original:].fillna(dt.iloc[:,N_original:].mean())\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# UMAP visualization\n","r__ = dt[\n","    ['Ess', 'GC'] +\n","    ['PC{}'.format(i) for i in range(1,31)] +\n","    feature_cols +\n","    feature_cols_for_PR].copy()\n","\n","tsne = TSNE()\n","\n","X_embedded = tsne.fit_transform(r__.iloc[:,1:])\n","X_embedded\n","tsneDf = pd.DataFrame(data = X_embedded, columns = ['t-SNE1', 't-SNE2'])\n","tsneDf['Ess'] = r__['Ess']\n","# print(tsneDf)\n","# plot if these information is enough to determine hubs\n","sns.jointplot(data=tsneDf, x=\"t-SNE1\", y=\"t-SNE2\", hue=\"Ess\", palette=['#416fec30', 'red'])\n","plt.legend(prop={'size':13})\n","plt.xlabel('t-SNE1', size=17)\n","plt.ylabel('t-SNE2', size=17)\n","plt.show()\n","\n","# ess and noness hubs are not distinguishable on a UMAP"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["############\n","# these dictionaries save the features to use, iterate thru them\n","PCs_to_use_dict = {\n","    '1,2':[1,2],\n","    '2,3':[2,3],\n","    '1,2,3':[1,2,3],\n","    '2,3,4':[2,3,4],\n","    'top4':[1,2,3,4],\n","    'top5':[1,2,3,4,5],\n","    'top10':list(np.arange(1,11)),\n","    'top20':list(np.arange(1,21)),\n","    'top30':list(np.arange(1,31)),\n","    'no':[]\n","}\n","\n","GC_to_use_dict = {\n","    'GC':['GC'],\n","    'noGC':[]\n","}\n","\n","c_features_to_use_dict = {\n","    'core histones':['H3K4me3.narrow.rep-1', 'H3K4me1.narrow.rep-1', 'H3K9me3.broad.rep-2', 'H3K27me3.broad.rep-2', 'H3K36me3.broad.rep-3', 'H3K27ac.narrow.rep-1'],\n","    'all histones':['H3K4me3.narrow.rep-1', 'H3K4me1.narrow.rep-1', 'H3K9me3.broad.rep-2', 'H3K27me3.broad.rep-2', 'H3K36me3.broad.rep-3', 'H3K27ac.narrow.rep-1',\n","                    'H3K9ac.narrow.rep-1','H4K20me1.narrow.rep-1','H3K4me2.narrow.rep-1'],\n","    'TFs':['CTCF.narrow.rep-1','RAD21.narrow.rep-1','SMC3.narrow.rep-1'],\n","    'no histone':[]\n","}\n","\n","g_feature_to_use_dict = {\n","    'gene annotations':['all_gene'],\n","    'lncRNAs':['lncRNA.Liu'],\n","    'essential genes':['ess_gene.Morgens', 'ess_gene.Wang'],\n","    'all genes':['ess_gene.Morgens', 'ess_gene.Wang', 'lncRNA.Liu', 'all_gene'],\n","    'no gene':[]\n","}\n","\n","atac_to_use_dict = {\n","    'ATAC':[ATAC],\n","    'no ATAC':[]\n","}\n","\n","pr_pk_to_use_dict = {\n","    'use_pk': lambda x:x,\n","    'use_pr': lambda x:['pr_mean_{}.scaled'.format(i) for i in x]+['pr_mean_default.scaled'],\n","    'use_pr_and_pk': lambda x:['pr_mean_{}.scaled'.format(i) for i in x]+['pr_mean_default.scaled']+x\n","}\n","# end of feature dicts\n","############\n","\n","\n","expr_hist = []\n","ROC_AUC_hist = []\n","rep_num = 10\n","\n","i = 0\n","# iterate thru combinations of features\n","for GC_name,GC_cols in GC_to_use_dict.items():\n","    for PC_name,PC_cols in PCs_to_use_dict.items():\n","        for histone_name, histone_cols in c_features_to_use_dict.items():\n","            for g_name, g_cols in g_feature_to_use_dict.items():\n","                for atac_name, atac_cols in atac_to_use_dict.items():\n","                    for p_name, p_function in pr_pk_to_use_dict.items():\n","                        expr_name = '+'.join(\n","                            [GC_name, PC_name+'PC', histone_name, g_feature_to_use_dict, \n","                            atac_to_use_dict, pr_pk_to_use_dict]\n","                            )\n","                        expr_cols = GC_cols + ['PC{}'.format(i_) for i_ in PC_cols]\n","                        expr_cols = expr_cols + (\n","                            p_function(histone_cols+g_cols+atac_cols)\n","                        )\n","                        if len(expr_cols)==0:\n","                            continue\n","                        i+=1\n","\n","\n","                        print('------------')\n","                        print(expr_name, i)\n","                        r__ = dt[['Ess']+expr_cols].copy() \n","                        \n","                        expr_hist.append(expr_name)\n","                        \n","                        ess_part = r__[r__['Ess']=='Ess'].copy().reset_index(drop=True)\n","                        noness_part = r__[r__['Ess']=='Noness'].copy().reset_index(drop=True)\n","\n","                        y_ess = np.ones((len(ess_part),))\n","                        X_ess = ess_part[expr_cols].values\n","                        y_noness = np.zeros((len(noness_part),))\n","                        X_noness = noness_part[expr_cols].values\n","\n","                        roc_hist_cur = []\n","                        pr_hist_cur = []\n","\n","                        for rep_id in range(rep_num):\n","                            # split 0.3:0.7 for ess and noness individually\n","                            X_train_ess, X_test_ess, y_train_ess, y_test_ess = train_test_split(X_ess, y_ess, test_size=0.3, \n","                                                                            random_state=0+20*rep_num,\n","                                                                            )\n","                            X_train_noness, X_test_noness, y_train_noness, y_test_noness = train_test_split(X_noness, y_noness, test_size=0.3, \n","                                                                                        random_state=0+1+20*rep_num,\n","                                                                                        )\n","\n","                            X_train = np.vstack((X_train_ess, X_train_noness))\n","                            X_test = np.vstack((X_test_ess, X_test_noness))\n","                            y_train = np.vstack((\n","                                np.array(y_train_ess).reshape((-1,1)), \n","                                np.array(y_train_noness).reshape((-1,1))\n","                                )).reshape(-1,)\n","                            y_test = np.vstack((\n","                                np.array(y_test_ess).reshape((-1,1)), \n","                                np.array(y_test_noness).reshape((-1,1)) \n","                                )).reshape(-1,)\n","\n","                            scaler = preprocessing.StandardScaler().fit(X_train)\n","                            X_train = scaler.transform(X_train)\n","                            X_test = scaler.transform(X_test)\n","\n","                            lg = LogisticRegression(class_weight = 'balanced', max_iter=1000, \n","                                                    random_state=0+2+20*rep_num, \n","                                                    # solver='liblinear',\n","                                                    )\n","                            lg.fit(X_train, y_train,  )\n","\n","                            # y_test_pred_prob = lg.predict_proba(X_test)[:,1] # proba of being predicted as ess\n","                            y_test_pred = lg.predict(X_test)\n","                            \n","                            # calc roc and pr\n","                            bac_this_model = balanced_accuracy_score(y_test, y_test_pred)\n","                            roc_hist_cur.append(bac_this_model)\n","                      \n","                        ROC_AUC_hist.append(np.mean(roc_hist_cur))\n","                        ROC_AUC_err_hist.append(np.std(roc_hist_cur))\n","\n","\n","tmp_dt = pd.DataFrame({\n","    'expr':expr_hist,\n","    'roc':ROC_AUC_hist,\n","    'roc_err':ROC_AUC_err_hist,\n","})\n","\n","tmp_dt\n","# the expectation is that roc score < 0.7, \n","# suggesting models trained on 1D epi-features cannot distinguish ess and noness hubs\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMJlRbe+fSPxnsWZqMtX/L5","gpuClass":"premium","machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
