{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"v2ukogkiGAjk"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import os, json\n","from scipy.optimize import curve_fit\n","from scipy.stats import pearsonr, spearmanr\n","\n","import networkx as nx\n","import seaborn as sns"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZmdKRc5orjoL"},"source":["this script finds all the hub pairs within 50-kb in all chr in K562 (all 26,148 hubs)\n","\n","1. find all hub pairs\n","\n","    load 26,148 list to find all hub pairs\n","\n","    only consider eligible hubs\n","    \n","    add ess info using the stringent list (1259), considering all GC cutoffs\n","\n","2. calc scaling factors\n","\n","3. set a cutoff and test lossely connected hub pairs on chr, find the proportion\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1685570442562,"user":{"displayName":"PY Wu","userId":"03546275994091638450"},"user_tz":420},"id":"JR026r9ATEUB","outputId":"c3d998f1-cf4a-481b-b5af-eb0d2f0eb3e0"},"outputs":[],"source":["# load 26148 hub dt\n","dt = pd.read_csv('resources/all_hubs.txt', sep='\\t')\n","dt = dt[['c', 's', 'str']]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjIdIe20Fzt4"},"outputs":[],"source":["# only consider eligible nodes (hubs) and eligible edges\n","node_meta_folder = 'PR-LR/node_meta.1'\n","edge_folder = 'PR-LR/eligible_edges'\n","\n","chrid_list =  ['chr{}'.format(i) for i in range(1,9)] + ['chr{}'.format(i) for i in range(10,22)] +['chrX', 'der9', 'phil22']\n","\n","str_list = []\n","for chrid in chrid_list:\n","    dt_ = pd.read_csv(os.path.join(node_meta_folder, '{}.txt'.format(chrid)), sep='\\t', )\n","    str_list.extend(list(dt_['str']))\n","\n","dt = dt[dt['str'].isin(str_list)].copy().reset_index(drop=True)\n","\n","# add meta (Ess) to dt\n","dt_ess = pd.read_csv('resources/all_hubs.for_LR.txt', sep='\\t')\n","\n","dt = dt.merge(dt_ess[['str', 'Ess']], on='str', how='left')\n","dt.fillna('UNK', inplace=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create FCNs and save to dict\n","FCN_dict = {}\n","for chrid in chrid_list:\n","    node_df = pd.read_csv('PR-LR/node_meta.1/{}.txt'.format(chrid),\n","                        sep='\\t')\n","\n","    edge_file = pd.read_csv('PR-LR/eligible_edges/{}.txt'.format(chrid),\n","                        sep='\\t', header=None)\n","    if len(edge_file.columns)==2:\n","        # str not in edge_file\n","        edge_file['str1'] = [chrid+'-']*len(edge_file) + edge_file[0].astype(str)\n","        edge_file['str2'] = [chrid+'-']*len(edge_file) + edge_file[1].astype(str)\n","        edge_file['c'] = chrid\n","        edge_file.columns = ['0', '1', 'str1', 'str2', 'c']\n","    else:\n","        edge_file.columns = ['0', '1', 'str1', 'str2', 'c'] # only phil22 and der9\n","\n","    nt = nx.from_pandas_edgelist(edge_file, 'str1', 'str2', create_using=nx.Graph) # construct undirected graph\n","    nt.add_nodes_from(list(node_df['str'])) # existing nodes are not affected\n","\n","    FCN_dict[chrid] = nt.copy()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaXNSfZKWcMN"},"outputs":[],"source":["# find hub pair in this part\n","def find_hub_pairs(chrid, chrnum, dt, G, max_dist=50000):\n","\n","    degree_dict = dict(G.degree())\n","    dt_ = dt[dt['str'].isin(degree_dict.keys())].copy()\n","\n","    s1_hist = []\n","    s2_hist = []\n","    str1_hist = []\n","    str2_hist = []\n","    dist_hist = []\n","    d1_hist = []\n","    d2_hist = []\n","    path_1_hist = []\n","    path_2_hist = []\n","    path_3_hist = []\n","    path_4_hist = []\n","\n","\n","    # use two pointer method\n","    srt_list = sorted(list(dt_['s'])) # hub start coord sorted\n","    # print(srt_list)\n","    left = 0\n","    right = 1\n","\n","    while left < len(srt_list) - 1 and right < len(srt_list):\n","        if left == right:\n","            right += 1\n","        s1 = srt_list[left]\n","        s2 = srt_list[right]\n","        diff = s2 - s1\n","\n","        if diff <= max_dist:\n","            s1_hist.append(s1)\n","            s2_hist.append(s2)\n","            dist_hist.append(diff)\n","\n","            node_1 = '{}-{}'.format(chrnum, s1)\n","            node_2 = '{}-{}'.format(chrnum, s2)\n","            str1_hist.append(node_1)\n","            str2_hist.append(node_2)\n","            \n","            d1_hist.append(degree_dict[node_1])\n","            d2_hist.append(degree_dict[node_2])\n","\n","            # print('{}-{}'.format(s1, s2))\n","            # print('{}-{}'.format(left, right))\n","            cnt=0\n","            for path in nx.all_simple_paths(G, node_1, node_2, cutoff=1):\n","                if len(path)>1:\n","                    cnt += 1\n","            path_1_hist.append(cnt)\n","\n","            cnt=0\n","            for path in nx.all_simple_paths(G, node_1, node_2, cutoff=2):\n","                if len(path)>2:\n","                    cnt += 1\n","            path_2_hist.append(cnt)\n","\n","            cnt=0\n","            for path in nx.all_simple_paths(G, node_1, node_2, cutoff=3):\n","                if len(path)>3:\n","                    cnt += 1\n","            path_3_hist.append(cnt)\n","\n","            cnt=0\n","            for path in nx.all_simple_paths(G, node_1, node_2, cutoff=4):\n","                if len(path)>4:\n","                    cnt += 1\n","            path_4_hist.append(cnt)\n","\n","            right += 1\n","\n","        else:\n","            left += 1\n","            right = left + 1\n","\n","\n","    results = pd.DataFrame({\n","        's1':s1_hist,\n","        's2':s2_hist,\n","        'str1':str1_hist,\n","        'str2':str2_hist,\n","        'dist':dist_hist,\n","        'd1':d1_hist,\n","        'd2':d2_hist,\n","        'path_1':path_1_hist,\n","        'path_2':path_2_hist,\n","        'path_3':path_3_hist,\n","        'path_4':path_4_hist,\n","\n","    })\n","    results = results[(results['d1']>0) & (results['d2']>0)].copy().reset_index(drop=True)\n","    results['c'] = chrid\n","\n","    ess_dict = dict(zip(\n","        list(dt['str']),\n","        list(dt['Ess'])\n","    ))\n","    results['Ess1'] = [ess_dict[i_] for i_ in results['str1']]\n","    results['Ess2'] = [ess_dict[i_] for i_ in results['str2']]\n","\n","    return results\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i, chr_num in enumerate(['chr{}'.format(i) for i in range(1,9)] + \n","                ['chr{}'.format(i) for i in range(10,22)] + \n","                ['chrX']):\n","    # print(chr_num)\n","    dt_this_chr = dt[dt['c']==chr_num].copy().reset_index(drop=True)\n","    if i==0:\n","        result_all = find_hub_pairs(chrid=chr_num, chrnum=chr_num, dt=dt_this_chr, G=FCN_dict[chr_num])\n","    else:\n","        result_all = pd.concat(\n","            (result_all, \n","            find_hub_pairs(chrid=chr_num, chrnum=chr_num, dt=dt_this_chr, G=FCN_dict[chr_num])\n","            ), ignore_index=True, \n","        )\n","\n","\n","transloc_dict_hg19 = {\n","    'ABL1': [133589268,133763062], # on chr9\n","    'BCR': [23522552,23660224]\n","}\n","\n","\n","chrid = 'der9'\n","dt_this_chr = dt[(dt['c']=='chr9') & (dt['s']<transloc_dict_hg19['ABL1'][0]-1000)].copy().reset_index(drop=True) # 'chr9-before', before the translocation site, allow 1kb extension\n","result_all = pd.concat(\n","            (result_all,\n","            find_hub_pairs(chrid=chrid, chrnum='chr9', dt=dt_this_chr, G=FCN_dict[chrid])\n","            ), ignore_index=True, \n","        )\n","\n","dt_this_chr = dt[(dt['c']=='chr22') & (dt['s']>transloc_dict_hg19['BCR'][1]+1000)].copy().reset_index(drop=True) # 'chr22-after'\n","result_all = pd.concat(\n","            (result_all,\n","            find_hub_pairs(chrid=chrid, chrnum='chr22', dt=dt_this_chr, G=FCN_dict[chrid])\n","            ), ignore_index=True, \n","        )\n","\n","\n","chrid = 'phil22'\n","dt_this_chr = dt[(dt['c']=='chr22') & (dt['s']<transloc_dict_hg19['BCR'][0]-1000)].copy().reset_index(drop=True) # 'chr22-before'\n","result_all = pd.concat(\n","            (result_all,\n","            find_hub_pairs(chrid=chrid, chrnum='chr22', dt=dt_this_chr, G=FCN_dict[chrid])\n","            ), ignore_index=True, \n","        )\n","\n","dt_this_chr = dt[(dt['c']=='chr9') & (dt['s']>transloc_dict_hg19['ABL1'][1]+1000)].copy().reset_index(drop=True) # 'chr9-after'\n","result_all = pd.concat(\n","            (result_all,\n","            find_hub_pairs(chrid=chrid, chrnum='chr9', dt=dt_this_chr, G=FCN_dict[chrid])\n","            ), ignore_index=True, \n","        )\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qbvPT9hq69N0"},"source":["scaling factor analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CoSLB94EWkqK"},"outputs":[],"source":["# normalize path counts, prepare for scaling fac analysis\n","result_all['deg_geom'] = (result_all['d1'] * result_all['d2']) ** (1/2) # geometric mean of degrees, use this to normalize data\n","result_all['ps1_norm'] = result_all['path_1'] / result_all['deg_geom'] # direct connectivity\n","result_all['ps2_norm'] = result_all['path_2'] / result_all['deg_geom'] # shared partners \n","result_all['ps3_norm'] = result_all['path_3'] / result_all['deg_geom']\n","result_all['ps4_norm'] = result_all['path_4'] / result_all['deg_geom']\n","\n","\n","result_all = result_all.sort_values('c').reset_index(drop=True)\n","result_all\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KwcaEjCrW8a-"},"outputs":[],"source":["# curve fit\n","def exp_curve(x, A, C):\n","\treturn A * np.exp(4*x) + C\n","\n","for j, (chrid, dt_) in enumerate(result_all.groupby('c')):\n","    print(chrid)\n","    ps_matrix = dt_[['ps{}_norm'.format(x_) for x_ in range(1,5)]].values\n","    scal_fac_hist = []\n","    for i in range(len(dt_)):\n","        (a, c), _ = curve_fit(\n","                        exp_curve, \n","                        [1,2,3,4], \n","                        list(ps_matrix[i, :]),\n","                        # [1, 1, np.median(cycles), 0], # initial guess for all params, very important\n","                        )\n","        # print(a,c)\n","        scal_fac_hist.append(a)\n","    \n","    dt__ = dt_.copy()\n","    dt__['scaling_fac'] = scal_fac_hist\n","    \n","    if j==0:\n","        resul_final = dt__.copy()\n","    else:\n","        resul_final = pd.concat((resul_final, dt__.copy()), ignore_index=True, )\n","\n","\n","resul_final"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPxaMLGxPrfL2plpB62DpV/","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
